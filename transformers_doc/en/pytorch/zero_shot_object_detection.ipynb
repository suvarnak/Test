{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSMXSEbDYEZs",
        "outputId": "c816d06a-68fc-4335-c364-b353ad91fecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu126)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets evaluate accelerate\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_M77zZhYEZv"
      },
      "source": [
        "# Zero-shot object detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOxhjnIIYEZw"
      },
      "source": [
        "Traditionally, models used for [object detection](https://huggingface.co/docs/transformers/main/en/tasks/object_detection) require labeled image datasets for training,\n",
        "and are limited to detecting the set of classes from the training data.\n",
        "\n",
        "Zero-shot object detection is a computer vision task to detect objects and their classes in images, without any\n",
        "prior training or knowledge of the classes. Zero-shot object detection models receive an image as input, as well\n",
        "as a list of candidate classes, and output the bounding boxes and labels where the objects have been detected.\n",
        "\n",
        "> [!NOTE]\n",
        "> Hugging Face houses many such [open vocabulary zero shot object detectors](https://huggingface.co/models?pipeline_tag=zero-shot-object-detection).\n",
        "\n",
        "In this guide, you will learn how to use such models:\n",
        "- to detect objects based on text prompts\n",
        "- for batch object detection\n",
        "- for image-guided object detection\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "pip install -q transformers\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub\n",
        "! hf auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TshVj1OZS9W",
        "outputId": "570ca4ae-3786-4125-bc6e-80dfbd731e11"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub) (2025.8.3)\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `hf-gated-access` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `hf-gated-access`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jsBOxwGYEZx"
      },
      "source": [
        "## Zero-shot object detection pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_oYu7__YEZx"
      },
      "source": [
        "The simplest way to try out inference with models is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a pipeline\n",
        "for zero-shot object detection from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-object-detection):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "02Ud2wEcYEZx",
        "outputId": "cfe4f8fc-2a2e-420f-a17f-16adce8b85cb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (ipython-input-1288193546.py, line 5)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1288193546.py\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\", token=os.environ.get(\"HF_GATED\")\u001b[0m\n\u001b[0m                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use any checkpoint from the hf.co/models?pipeline_tag=zero-shot-object-detection\n",
        "checkpoint = \"iSEE-Laboratory/llmdet_large\"\n",
        "detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\", token=os.environ.get(\"HF_GATED\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TExJ5aFaYEZx"
      },
      "source": [
        "Next, choose an image you'd like to detect objects in. Here we'll use the image of astronaut Eileen Collins that is\n",
        "a part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html) Great Images dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk-EtDRHYEZx"
      },
      "outputs": [],
      "source": [
        "from transformers.image_utils import load_image\n",
        "\n",
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\"\n",
        "image = load_image(url)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0Ml102YYEZx"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\" alt=\"Astronaut Eileen Collins\"/>\n",
        "</div>\n",
        "\n",
        "Pass the image and the candidate object labels to look for to the pipeline.\n",
        "Here we pass the image directly; other suitable options include a local path to an image or an image url. We also pass text descriptions for all items we want to query the image for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0y9_b74YEZy"
      },
      "outputs": [],
      "source": [
        "predictions = detector(\n",
        "    image,\n",
        "    candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\n",
        "    threshold=0.45,\n",
        ")\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd_xxNpUYEZy"
      },
      "source": [
        "Let's visualize the predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZJK-nyhYEZy"
      },
      "outputs": [],
      "source": [
        "from PIL import ImageDraw\n",
        "\n",
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "for prediction in predictions:\n",
        "    box = prediction[\"box\"]\n",
        "    label = prediction[\"label\"]\n",
        "    score = prediction[\"score\"]\n",
        "\n",
        "    xmin, ymin, xmax, ymax = box.values()\n",
        "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
        "    draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L85_SV1DYEZy"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_2.png\" alt=\"Visualized predictions on NASA image\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuwllOvqYEZy"
      },
      "source": [
        "## Text-prompted zero-shot object detection by hand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd45fHMlYEZy"
      },
      "source": [
        "Now that you've seen how to use the zero-shot object detection pipeline, let's replicate the same result manually.\n",
        "\n",
        "Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/docs/transformers/main/en/tasks/hf.co/iSEE-Laboratory/llmdet_large).\n",
        "Here we'll use the same checkpoint as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ygoer49YEZy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint, device_map=\"auto\")\n",
        "processor = AutoProcessor.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjjLgNKFYEZy"
      },
      "source": [
        "Let's take a different image to switch things up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvVOORihYEZy"
      },
      "outputs": [],
      "source": [
        "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\"\n",
        "image = load_image(url)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBOGa7IdYEZz"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\n",
        "</div>\n",
        "\n",
        "Use the processor to prepare the inputs for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGprKnwBYEZz"
      },
      "outputs": [],
      "source": [
        "text_labels = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\n",
        "inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")to(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEgjQj-9YEZz"
      },
      "source": [
        "Pass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\n",
        "feeding them to the model, you need to use the `post_process_object_detection` method to make sure the predicted bounding\n",
        "boxes have the correct coordinates relative to the original image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgg1dWE2YEZz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "results = processor.post_process_grounded_object_detection(\n",
        "   outputs, threshold=0.50, target_sizes=[(image.height, image.width)], text_labels=text_labels,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjAy8NbKYEZz"
      },
      "outputs": [],
      "source": [
        "draw = ImageDraw.Draw(image)\n",
        "\n",
        "scores = results[\"scores\"]\n",
        "text_labels = results[\"text_labels\"]\n",
        "boxes = results[\"boxes\"]\n",
        "\n",
        "for box, score, text_label in zip(boxes, scores, text_labels):\n",
        "    xmin, ymin, xmax, ymax = box\n",
        "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
        "    draw.text((xmin, ymin), f\"{text_label}: {round(score.item(),2)}\", fill=\"white\")\n",
        "\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aZLXjtxYEZz"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAFRzTAXYEZz"
      },
      "source": [
        "## Batch processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N-ioqkwYEZz"
      },
      "source": [
        "You can pass multiple sets of images and text queries to search for different (or same) objects in several images.\n",
        "Let's use both an astronaut image and the beach image together.\n",
        "For batch processing, you should pass text queries as a nested list to the processor and images as lists of PIL images,\n",
        "PyTorch tensors, or NumPy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u5TU74CYEZz"
      },
      "outputs": [],
      "source": [
        "url1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\"\n",
        "url2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\"\n",
        "images = [load_image(url1), load_image(url2)]\n",
        "text_queries = [\n",
        "    [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\n",
        "    [\"hat\", \"book\", \"sunglasses\", \"camera\", \"can\"],\n",
        "]\n",
        "inputs = processor(text=text_queries, images=images, return_tensors=\"pt\", padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj9wfbcBYEZz"
      },
      "source": [
        "Previously for post-processing you passed the single image's size as a tensor, but you can also pass a tuple, or, in case\n",
        "of several images, a list of tuples. Let's create predictions for the two examples, and visualize the second one (`image_idx = 1`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_4lREEoYEZz"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "target_sizes = [(image.height, image.width) for image in images]\n",
        "results = processor.post_process_grounded_object_detection(\n",
        "    outputs, threshold=0.3, target_sizes=target_sizes, text_labels=text_labels,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs34d7AQYEZz"
      },
      "source": [
        "Let's visualize the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdxoNFIYYEZz"
      },
      "outputs": [],
      "source": [
        "image_idx = 1\n",
        "draw = ImageDraw.Draw(images[image_idx])\n",
        "\n",
        "scores = results[image_idx][\"scores\"].tolist()\n",
        "text_labels = results[image_idx][\"text_labels\"]\n",
        "boxes = results[image_idx][\"boxes\"].tolist()\n",
        "\n",
        "for box, score, text_label in zip(boxes, scores, text_labels):\n",
        "    xmin, ymin, xmax, ymax = box\n",
        "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
        "    draw.text((xmin, ymin), f\"{text_label}: {round(score,2)}\", fill=\"white\")\n",
        "\n",
        "images[image_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iy2FOA3rYEZz"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png\" alt=\"Beach photo with detected objects\"/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-MItQudYEZz"
      },
      "source": [
        "## Image-guided object detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yOJAY4YYEZz"
      },
      "source": [
        "In addition to zero-shot object detection with text queries, models like [OWL-ViT](https://huggingface.co/collections/ariG23498/owlvit-689b0d0872a7634a6ea17ae7) and [OWLv2](https://huggingface.co/collections/ariG23498/owlv2-689b0d27bd7d96ba3c7f7530) offers image-guided object detection. This means you can use an image query to find similar\n",
        "objects in the target image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfVhqZFHYEZz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
        "\n",
        "checkpoint = \"google/owlv2-base-patch16-ensemble\"\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint, device_map=\"auto\")\n",
        "processor = AutoProcessor.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcHDn9SEYEZ0"
      },
      "source": [
        "Unlike text queries, only a single example image is allowed.\n",
        "\n",
        "Let's take an image with two cats on a couch as a target image, and an image of a single cat\n",
        "as a query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-wWaiLpYEZ0"
      },
      "outputs": [],
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image_target = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "query_url = \"http://images.cocodataset.org/val2017/000000524280.jpg\"\n",
        "query_image = Image.open(requests.get(query_url, stream=True).raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGbQbnjAYEZ0"
      },
      "source": [
        "Let's take a quick look at the images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00gfG-OgYEZ0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_target)\n",
        "ax[1].imshow(query_image)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTAnmkyuYEZ3"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_5.png\" alt=\"Cats\"/>\n",
        "</div>\n",
        "\n",
        "In the preprocessing step, instead of text queries, you now need to use `query_images`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT18uJpVYEZ3"
      },
      "outputs": [],
      "source": [
        "inputs = processor(images=image_target, query_images=query_image, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JtTcScZYEZ3"
      },
      "source": [
        "For predictions, instead of passing the inputs to the model, pass them to [image_guided_detection()](https://huggingface.co/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection.image_guided_detection). Draw the predictions\n",
        "as before except now there are no labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fhjhxWYYEZ3"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model.image_guided_detection(**inputs)\n",
        "    target_sizes = torch.tensor([image_target.size[::-1]])\n",
        "    results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]\n",
        "\n",
        "draw = ImageDraw.Draw(image_target)\n",
        "\n",
        "scores = results[\"scores\"].tolist()\n",
        "boxes = results[\"boxes\"].tolist()\n",
        "\n",
        "for box, score in zip(boxes, scores):\n",
        "    xmin, ymin, xmax, ymax = box\n",
        "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\n",
        "\n",
        "image_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjspjSwoYEZ3"
      },
      "source": [
        "<div class=\"flex justify-center\">\n",
        "     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_6.png\" alt=\"Cats with bounding boxes\"/>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}