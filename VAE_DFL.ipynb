{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE DFL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suvarnak/Test/blob/master/VAE_DFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvvR_NmECpsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "import os \n",
        "\n",
        "EPOCHS = 10\n",
        "CUDA = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 128\n",
        "SEED = 1\n",
        "LOG_INTERVAL = 10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cRK9aujC64L",
        "colab_type": "code",
        "outputId": "9f3bdac7-4254-4213-e0b8-1c49a8e52a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3588110.31it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 55697.40it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:01, 915968.27it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21124.99it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRsA_9FsBHwO",
        "colab_type": "code",
        "outputId": "e861b28c-92f7-4c63-cc19-d9c3dff8df2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "folder = \"results\"\n",
        "\n",
        "if not os.path.exists(folder):\n",
        "    os.mkdir(folder)\n",
        "    print(\"Directory \" , folder ,  \" Created \")\n",
        "else:    \n",
        "    print(\"Directory \" , folder ,  \" already exists\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory  results  Created \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N5obFonGuvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import shutil\n",
        "# shutil.rmtree(\"results\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJIEE2GNClbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        # fc1, fc21 and fc22 are used by the encoder.\n",
        "        # fc1 takes a vectorized MNIST image as input\n",
        "        # fc21 and fc22 are both attached to the activation output of fc1 (using ReLU).\n",
        "        # fc21 outputs the means, and fc22 the log-variances of\n",
        "        # each component of th 20-dimensional latent Gaussian.\n",
        "        self.fc1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3,\n",
        "                  stride=(2,2))\n",
        "        self.fc1_normalized = nn.BatchNorm2d(32)\n",
        "        self.fc2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,\n",
        "                  stride=(2,2))\n",
        "        self.fc2_normalized = nn.BatchNorm2d(64)\n",
        "        self.fc21 = nn.Linear(6*6*64, self.latent_dim)\n",
        "        self.fc22 = nn.Linear(6*6*64, self.latent_dim)\n",
        "        # fc3 and fc4 are connected in series as the decoder.\n",
        "        # fc3 takes a realization from the latent space as input\n",
        "        # and the decoder generates a vectorized 28x28 image.\n",
        "        # The output of fc3 passes through a ReLU,\n",
        "        # while fc4 uses a sigmoid in order to output a probability for each pixel\n",
        "        self.fc3 = nn.Linear(self.latent_dim, 6*6*64)\n",
        "        self.fc4 = nn.ConvTranspose2d(in_channels=64, out_channels=32,\n",
        "                                     kernel_size=3, stride=(2,2))\n",
        "        self.fc4_normalized = nn.BatchNorm2d(32)\n",
        "        self.fc5 = nn.ConvTranspose2d(in_channels=32, out_channels=28,\n",
        "                                     kernel_size=3, stride=(2,2))\n",
        "        self.fc5_normalized = nn.BatchNorm2d(28)\n",
        "        self.fc6 = nn.ConvTranspose2d(in_channels=28, out_channels=1,\n",
        "                                     kernel_size=2, stride=(1,1))\n",
        "\n",
        "    # TODO: Implement the following four functions.  Note that they should be able to accept arguments containing stacked information for multiple observations\n",
        "    # e.g. a minibatch rather than a single observation.  Your solution will need to handle this.  If you treat the arguments as\n",
        "    # representing a single observation in your logic, in most cases broadcasting will do the rest of the job automatically for you.\n",
        "    def encode(self, x):\n",
        "        # This should return the outputs of fc21 and fc22 as a tuple\n",
        "        hidden1 = F.selu(self.fc1(x))\n",
        "        hidden1 = self.fc1_normalized(hidden1)       \n",
        "        hidden2 = F.selu(self.fc2(hidden1))\n",
        "        hidden2 = self.fc2_normalized(hidden2)               \n",
        "        flatten = hidden2.view(-1, 6*6*64)\n",
        "        mu = self.fc21(flatten)\n",
        "        logvar = self.fc22(flatten)\n",
        "        return (mu, logvar)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        # This should sample vectors from an isotropic Gaussian, and use these to generate\n",
        "        # and return observations with a mean vectors from mu, and log-variances of log-var\n",
        "        latent_dim = mu.shape\n",
        "        loc = torch.zeros(latent_dim)\n",
        "        covariance_matrix = torch.diag_embed(torch.ones(latent_dim))\n",
        "        gaussian = torch.distributions.MultivariateNormal(loc, covariance_matrix)\n",
        "        epsilon = gaussian.sample().to(device)\n",
        "        latent_vector = mu + logvar * epsilon\n",
        "        return latent_vector\n",
        "\n",
        "    def decode(self, z):\n",
        "        # Pass z through the decoder. For each 20-dimensional latent realization, there should be a 784-dimensional vector of\n",
        "        #probabilities generated, one per pixel\n",
        "        hidden1 = F.selu(self.fc3(z))\n",
        "        hidden1 = hidden1.view(-1,64,6,6)\n",
        "        hidden2 = F.selu(self.fc4(hidden1))\n",
        "        hidden2 = self.fc4_normalized(hidden2)\n",
        "        hidden3 = F.selu(self.fc5(hidden2))\n",
        "        hidden3 = self.fc5_normalized(hidden3)        \n",
        "        hidden4 = self.fc6(hidden3)\n",
        "        realizations = torch.sigmoid(hidden4)\n",
        "#         import pdb; pdb.set_trace()\n",
        "        return realizations\n",
        "\n",
        "    def forward(self, x):\n",
        "        # For each observation in x:\n",
        "        # 1. Pass it through the encoder to get predicted variational distribution parameters\n",
        "        # 2. Reparameterize an isotropic Gaussian with these parameters to get sample latent variable realizations\n",
        "        # 3. Pass the realization through the decoder to get predicted pixel probabilities\n",
        "        # Return a tuple with 3 elements: (a) the predicted pixel probabilities, (b) the predicted variational means, and (c) the predicted variational log-variances\n",
        "        #x = x.view(-1,784) # Reshape x to provide suitable inputs to the encoder\n",
        "        mu, logvar = self.encode(x)\n",
        "        latent_vector = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decode(latent_vector)\n",
        "        return (recon_x, mu, logvar)\n",
        "        \n",
        "        \n",
        "latent_dimension=100\n",
        "model = VAE(latent_dimension).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# TODO: Implement this loss function\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    # The loss should be (an estimate of) the negative ELBO - remember we wish to maximise the ELBO - but the ELBO can be written in a number of forms.\n",
        "    # In this case, the prior for the latent variable and the variational posterior are both Gaussians, and we will exploit this.\n",
        "    # Specifically, we can analytically calculate a part of the ELBO, and only use Monte Carlo estimation for the rest.\n",
        "    # 1. We use the form of the ELBO which includes a KL divergence between the latent prior and the variational family\n",
        "    # - see the form at the bottom of page 6 of Blei et al's \"Variational Inference: A Review for Statisticians\".\n",
        "    # 2. In this case, the expression for the relevant KL divergence can be obtained from Exercise (e) in Week 1.\n",
        "    #\n",
        "    # The other term is the expected conditional log-likelihood, which is estimated using a single Monte-Carlo sample.\n",
        "    # For the log-likelihood, one evaluates the probability of observing an input point given the \"conditional distribution\" for\n",
        "    # observations output by the network - in this case, each pixel is independently Bernoulli with parameter equal to the output probability.\n",
        "    # You may find torch.nn.functional's binary_cross_entropy function useful here.\n",
        "    #\n",
        "    # Additional: the extraction of the KL divergence as above reduces the variance.  Investigate the effect of directly estimating\n",
        "    # the full ELBO term for each observation with a single Monte Carlo sample.\n",
        "    #\n",
        "    # You may find torch.nn.functional's binary_cross_entropy function useful.\n",
        "    #\n",
        "    # Return a single value accumulating the loss over the whole batch.\n",
        "    #\n",
        "    # Arguments:\n",
        "    # x is the batch of observations\n",
        "    # recon_x, mu, and logvar are the outputs of forward(x) (above) - see the usage below\n",
        "    #x = x.view(-1,784) # Reshape x to provide suitable inputs to the encoder\n",
        "   \n",
        "    # Computing the KL term\n",
        "    mu_square = mu.pow(2)\n",
        "    var = logvar.exp()\n",
        "    KL = .5 * torch.sum((var + mu_square - logvar - 1), dim=-1)\n",
        "    KL = KL.mean()    \n",
        "    # Computing the conditional log likelihood expectation\n",
        "#    covariance_matrix = torch.diag_embed(logvar.exp())\n",
        "#    variational_posterior = torch.distributions.MultivariateNormal(mu, covariance_matrix)\n",
        "#    z_sample = variational_posterior.sample()\n",
        "    log_loss = -F.binary_cross_entropy(recon_x, x, reduction=\"mean\")    \n",
        "    ELBO = log_loss + KL   \n",
        "    return ELBO\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0zVcrPoBe4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "          epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EARm576zBjOG",
        "colab_type": "code",
        "outputId": "22927c65-bda3-44ba-de93-0fa19188f922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        train(epoch)\n",
        "        test(epoch)\n",
        "        with torch.no_grad():\n",
        "            sample = torch.randn(64, latent_dimension).to(device)\n",
        "            sample = model.decode(sample).cpu()\n",
        "            save_image(sample.view(64, 1, 28, 28),\n",
        "                       'results/sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.185550\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.018197\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: -0.034112\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: -0.064381\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: -0.082728\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -0.098305\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: -0.111410\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: -0.123618\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: -0.134580\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: -0.139404\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -0.149569\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: -0.157201\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: -0.159768\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: -0.169223\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: -0.168684\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -0.151239\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: -0.161904\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: -0.173331\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: -0.180397\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: -0.181344\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -0.156702\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: -0.182597\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: -0.184194\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: -0.187294\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: -0.187817\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -0.187836\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: -0.179947\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: -0.186667\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: -0.189970\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: -0.192284\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -0.190637\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: -0.191844\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: -0.190860\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: -0.190813\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: -0.192643\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -0.190596\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: -0.166362\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: -0.178249\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: -0.186886\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: -0.193471\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -0.195511\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: -0.196590\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: -0.198218\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: -0.198431\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: -0.196734\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -0.194642\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: -0.195214\n",
            "====> Epoch: 1 Average loss: -0.1601\n",
            "====> Test set loss: -0.1973\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: -0.196535\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: -0.194548\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: -0.192909\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: -0.196489\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: -0.196092\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: -0.197147\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: -0.197051\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: -0.195829\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: -0.193368\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: -0.194845\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: -0.195662\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: -0.195544\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: -0.196016\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: -0.193220\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: -0.193466\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: -0.196026\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: -0.194281\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: -0.194861\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: -0.194089\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: -0.193586\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: -0.190841\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: -0.191847\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: -0.189675\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: -0.194040\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: -0.191251\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: -0.192545\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: -0.191771\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: -0.191936\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: -0.190871\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: -0.191732\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: -0.191527\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: -0.191043\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: -0.192861\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: -0.192057\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: -0.192428\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: -0.192050\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: -0.191530\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: -0.192187\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: -0.192111\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: -0.193034\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: -0.194027\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: -0.194239\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: -0.194736\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: -0.193489\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: -0.195086\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: -0.193138\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: -0.195401\n",
            "====> Epoch: 2 Average loss: -0.1938\n",
            "====> Test set loss: -0.1971\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: -0.196331\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: -0.194494\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: -0.194534\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: -0.195861\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: -0.198116\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: -0.196687\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: -0.197984\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: -0.197012\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: -0.196861\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: -0.197537\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: -0.195782\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: -0.198202\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: -0.197621\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: -0.198541\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: -0.198532\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: -0.198997\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: -0.198918\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: -0.199447\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: -0.200490\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: -0.199553\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: -0.199019\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: -0.199036\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: -0.200205\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: -0.200739\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: -0.200077\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: -0.199983\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: -0.202070\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: -0.202533\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: -0.203055\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: -0.202627\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: -0.202702\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: -0.202239\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: -0.205695\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: -0.203126\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: -0.203034\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: -0.204488\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: -0.204373\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: -0.204441\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: -0.204701\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: -0.205345\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -0.206454\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: -0.204543\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: -0.205044\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: -0.206428\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: -0.206516\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: -0.205972\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: -0.207040\n",
            "====> Epoch: 3 Average loss: -0.2010\n",
            "====> Test set loss: -0.2087\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: -0.206689\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: -0.206659\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: -0.207433\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: -0.208166\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: -0.208257\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: -0.206645\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: -0.208139\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: -0.209099\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: -0.209579\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: -0.208419\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: -0.209533\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: -0.209606\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: -0.209846\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: -0.210418\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: -0.209246\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: -0.209027\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: -0.210463\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: -0.208149\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: -0.210297\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: -0.210879\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: -0.211078\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: -0.210951\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: -0.211209\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: -0.211257\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: -0.211183\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: -0.211634\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: -0.211359\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: -0.211109\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: -0.211063\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: -0.211965\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -0.212499\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: -0.212540\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: -0.212392\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: -0.212139\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: -0.211915\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: -0.212263\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: -0.212280\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: -0.211797\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: -0.211491\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: -0.212060\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -0.212310\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: -0.212903\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: -0.212324\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: -0.212781\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: -0.212419\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: -0.212818\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: -0.212414\n",
            "====> Epoch: 4 Average loss: -0.2108\n",
            "====> Test set loss: -0.2140\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: -0.211824\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: -0.211837\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: -0.212086\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: -0.211279\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: -0.212859\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: -0.212687\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: -0.212592\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: -0.212022\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: -0.212296\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: -0.212255\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: -0.211642\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: -0.212933\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: -0.212019\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: -0.211516\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: -0.212064\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: -0.212860\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: -0.211823\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: -0.212895\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: -0.213272\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: -0.212370\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -0.212979\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: -0.213404\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: -0.211938\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: -0.213292\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: -0.212852\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -0.213109\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: -0.213045\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: -0.212993\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: -0.212605\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: -0.212618\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -0.212651\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: -0.212362\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: -0.211438\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: -0.212126\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: -0.211359\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: -0.212615\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: -0.211816\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: -0.212469\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: -0.212337\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: -0.212045\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -0.212997\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: -0.211506\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: -0.212556\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: -0.213122\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: -0.212952\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: -0.213154\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: -0.212482\n",
            "====> Epoch: 5 Average loss: -0.2124\n",
            "====> Test set loss: -0.2173\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: -0.212498\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: -0.212806\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: -0.213309\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: -0.212318\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: -0.211518\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: -0.211081\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: -0.210676\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: -0.212208\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: -0.211677\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: -0.211748\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -0.212269\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: -0.212448\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: -0.212190\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: -0.210595\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: -0.213213\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: -0.211488\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: -0.212119\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: -0.211766\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: -0.211739\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: -0.212246\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -0.211661\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: -0.211945\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: -0.211184\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: -0.211284\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: -0.211434\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -0.210946\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: -0.211468\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: -0.211101\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: -0.210493\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: -0.211228\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -0.210642\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: -0.211792\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: -0.211625\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: -0.211363\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: -0.212077\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: -0.212743\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: -0.211595\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: -0.212336\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: -0.211022\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: -0.211589\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -0.211596\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: -0.210378\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: -0.210993\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: -0.212896\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: -0.211890\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: -0.211889\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: -0.211280\n",
            "====> Epoch: 6 Average loss: -0.2119\n",
            "====> Test set loss: -0.2156\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: -0.211636\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: -0.212215\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: -0.211527\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: -0.211972\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: -0.210711\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: -0.211407\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: -0.211751\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: -0.212162\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: -0.211624\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: -0.211035\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: -0.210861\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: -0.211080\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: -0.212139\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: -0.211427\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: -0.210504\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: -0.211212\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: -0.209971\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: -0.212665\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: -0.210513\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: -0.210815\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -0.211077\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: -0.212105\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: -0.212093\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: -0.211435\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: -0.210564\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: -0.210091\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: -0.210601\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: -0.214003\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: -0.212953\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: -0.213106\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: -0.211675\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: -0.210566\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: -0.212005\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: -0.209829\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: -0.211362\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: -0.211309\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: -0.210908\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: -0.210476\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: -0.210445\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: -0.211058\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -0.211620\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: -0.210793\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: -0.210399\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: -0.210383\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: -0.211310\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: -0.210448\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: -0.209886\n",
            "====> Epoch: 7 Average loss: -0.2112\n",
            "====> Test set loss: -0.2152\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: -0.212069\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: -0.210491\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: -0.211084\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: -0.209984\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: -0.210385\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: -0.210690\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: -0.210768\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: -0.209880\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: -0.209986\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: -0.210839\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: -0.210866\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: -0.210392\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: -0.210449\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: -0.210855\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: -0.211741\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: -0.210623\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: -0.209379\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: -0.211099\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: -0.210293\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: -0.211044\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -0.210445\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: -0.210365\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: -0.210057\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: -0.210007\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: -0.211191\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: -0.210067\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: -0.210613\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: -0.209844\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: -0.209839\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: -0.210561\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: -0.209947\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: -0.208820\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: -0.209995\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: -0.210149\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: -0.210242\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: -0.210464\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: -0.209487\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: -0.209877\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: -0.209310\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: -0.210518\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -0.210008\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: -0.210574\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: -0.210886\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: -0.210529\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: -0.210269\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: -0.209548\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: -0.210050\n",
            "====> Epoch: 8 Average loss: -0.2105\n",
            "====> Test set loss: -0.2119\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: -0.209424\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: -0.211148\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: -0.209035\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: -0.210205\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: -0.210177\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: -0.210380\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: -0.209307\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: -0.209875\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: -0.210182\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: -0.209387\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: -0.210267\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: -0.210349\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: -0.209938\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: -0.209564\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: -0.209968\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: -0.210091\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: -0.209569\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: -0.209928\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: -0.209804\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: -0.209820\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -0.208886\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: -0.210099\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: -0.209983\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: -0.209679\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: -0.209579\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: -0.209947\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: -0.209558\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: -0.208983\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: -0.209641\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: -0.210324\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: -0.209758\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: -0.209179\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: -0.209360\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: -0.209719\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: -0.209664\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: -0.209893\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: -0.209400\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: -0.210282\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: -0.209780\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: -0.209501\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -0.209167\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: -0.209524\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: -0.209979\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: -0.210378\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: -0.209796\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: -0.208859\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: -0.210462\n",
            "====> Epoch: 9 Average loss: -0.2099\n",
            "====> Test set loss: -0.2122\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: -0.209982\n",
            "Train Epoch: 10 [1280/60000 (2%)]\tLoss: -0.209139\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: -0.209666\n",
            "Train Epoch: 10 [3840/60000 (6%)]\tLoss: -0.210425\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: -0.209731\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: -0.209967\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: -0.210354\n",
            "Train Epoch: 10 [8960/60000 (15%)]\tLoss: -0.209291\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: -0.209820\n",
            "Train Epoch: 10 [11520/60000 (19%)]\tLoss: -0.209160\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: -0.210022\n",
            "Train Epoch: 10 [14080/60000 (23%)]\tLoss: -0.209515\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: -0.209803\n",
            "Train Epoch: 10 [16640/60000 (28%)]\tLoss: -0.208711\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: -0.209445\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: -0.209424\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: -0.209136\n",
            "Train Epoch: 10 [21760/60000 (36%)]\tLoss: -0.209371\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: -0.209734\n",
            "Train Epoch: 10 [24320/60000 (41%)]\tLoss: -0.209196\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: -0.208804\n",
            "Train Epoch: 10 [26880/60000 (45%)]\tLoss: -0.209257\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: -0.209467\n",
            "Train Epoch: 10 [29440/60000 (49%)]\tLoss: -0.209433\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: -0.209227\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: -0.209066\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: -0.209558\n",
            "Train Epoch: 10 [34560/60000 (58%)]\tLoss: -0.209791\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: -0.209981\n",
            "Train Epoch: 10 [37120/60000 (62%)]\tLoss: -0.209363\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: -0.209259\n",
            "Train Epoch: 10 [39680/60000 (66%)]\tLoss: -0.208825\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: -0.209761\n",
            "Train Epoch: 10 [42240/60000 (70%)]\tLoss: -0.209019\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: -0.208837\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: -0.208364\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: -0.209555\n",
            "Train Epoch: 10 [47360/60000 (79%)]\tLoss: -0.209207\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: -0.209081\n",
            "Train Epoch: 10 [49920/60000 (83%)]\tLoss: -0.208681\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: -0.209121\n",
            "Train Epoch: 10 [52480/60000 (87%)]\tLoss: -0.209568\n",
            "Train Epoch: 10 [53760/60000 (90%)]\tLoss: -0.209274\n",
            "Train Epoch: 10 [55040/60000 (92%)]\tLoss: -0.209323\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: -0.209422\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: -0.209186\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: -0.208792\n",
            "====> Epoch: 10 Average loss: -0.2095\n",
            "====> Test set loss: -0.2115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtmIKjyIBnNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}